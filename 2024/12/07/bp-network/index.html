<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"volcano-x.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Introduction 近十年来，深度学习在各个人工智能领域，如自然语言处理，计算机视觉，图结构数据等，均取得了突破性的进展。当前深度学习研究的基本范式为：  网络结构：设计基于神经网络的特征学习器； 目标函数：设计具体的目标函数； 优化算法：基于后向传播的梯度优化算法对网络参数进行学习；  为了更深刻地理解深度学习的优化基础，本文从底层开始，对神经网络的基本原理进行探索。首先，我们考虑如下的神">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络的 Back Propagation 优化算法的数学原理推导及其 python 实现">
<meta property="og:url" content="https://volcano-x.github.io/2024/12/07/bp-network/index.html">
<meta property="og:site_name" content="VolcanoX">
<meta property="og:description" content="Introduction 近十年来，深度学习在各个人工智能领域，如自然语言处理，计算机视觉，图结构数据等，均取得了突破性的进展。当前深度学习研究的基本范式为：  网络结构：设计基于神经网络的特征学习器； 目标函数：设计具体的目标函数； 优化算法：基于后向传播的梯度优化算法对网络参数进行学习；  为了更深刻地理解深度学习的优化基础，本文从底层开始，对神经网络的基本原理进行探索。首先，我们考虑如下的神">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-12-07T01:51:34.000Z">
<meta property="article:modified_time" content="2024-12-07T03:14:42.190Z">
<meta property="article:author" content="Jiacan Zheng">
<meta property="article:tag" content="neural network">
<meta property="article:tag" content="science">
<meta property="article:tag" content="technology">
<meta property="article:tag" content="math">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://volcano-x.github.io/2024/12/07/bp-network/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>神经网络的 Back Propagation 优化算法的数学原理推导及其 python 实现 | VolcanoX</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">VolcanoX</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">除了自己的无知，我什么都不懂。   —— 苏格拉底</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">3</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://volcano-x.github.io/2024/12/07/bp-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zjc-512x512.png">
      <meta itemprop="name" content="Jiacan Zheng">
      <meta itemprop="description" content="Great Truths Are Always Simple.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VolcanoX">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          神经网络的 Back Propagation 优化算法的数学原理推导及其 python 实现
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2024-12-07 09:51:34 / Modified: 11:14:42" itemprop="dateCreated datePublished" datetime="2024-12-07T09:51:34+08:00">2024-12-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Technology/" itemprop="url" rel="index"><span itemprop="name">Technology</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Technology/Math/" itemprop="url" rel="index"><span itemprop="name">Math</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="introduction">Introduction</h2>
<p>近十年来，深度学习在各个人工智能领域，如自然语言处理，计算机视觉，图结构数据等，均取得了突破性的进展。当前深度学习研究的基本范式为：</p>
<ul>
<li><strong>网络结构</strong>：设计基于神经网络的特征学习器；</li>
<li><strong>目标函数</strong>：设计具体的目标函数；</li>
<li><strong>优化算法</strong>：基于后向传播的梯度优化算法对网络参数进行学习；</li>
</ul>
<p>为了更深刻地理解深度学习的优化基础，本文从底层开始，对神经网络的基本原理进行探索。首先，我们考虑如下的神经网络设计：</p>
<ul>
<li><strong>网络结构</strong>：全连接网络结构；</li>
<li><strong>目标函数</strong>：交叉熵目标函数(分类问题)，最小二乘目标函数(回归问题)；</li>
<li><strong>优化算法</strong>：批量梯度下降算法；</li>
</ul>
<p>在理论上，我们对全连接神经网络在数学上进行形式化，根据链式法则给出其网络参数的导数的简洁矩阵形式，最后展示如何利用批量梯度下降进行优化。在实验上，我们利用
python 的 numpy 库从源头上完全实现了整个后向传播优化算法，并在 mnist
手写数字集上测试了代码的可靠性。</p>
<h2 id="notation">Notation</h2>
<p>推导神经网络的梯度公式是一个繁琐的过程，因为非常有必要引入简洁统一的记号，本文使用的记号和规定如下：</p>
<p>我们考虑的全连接网络有 <span class="math inline">\(l+1\)</span>
个节点层，从左往右，依次为：</p>
<ul>
<li>第 0 层称为输入层 (input layer);</li>
<li>第 <span class="math inline">\(l\)</span> 层称为输出层 (output
layer);</li>
<li>中间的层 <span class="math inline">\(0 &lt; k &lt; l\)</span>
称为隐藏层 (hidden layer)。</li>
</ul>
<p>当我们讨论第 k 层时，我们指的是从第 k-1 层节点到 第 k
层节点之间处理过程，其输入是一个特征矩阵 <span
class="math inline">\(\mathbf{Y}^{(k-1)} \in \mathbb{R}^{N\times
n_{k-1}}\)</span>，输出为一个新的特征矩阵 <span
class="math inline">\(\mathbf{Y}^{(k)} \in \mathbb{R}^{m\times
n_{k}}\)</span>，每一行表示一个数据样本。第 k 层的节点数指的是 <span
class="math inline">\(n_k\)</span>，特别的，有 <span
class="math inline">\(n_0 = d\)</span> (原始数据的维度)， <span
class="math inline">\(n_l = r\)</span> (输出层的维度). 特别地，对于第 0
层，我们有 <span class="math inline">\(\mathbf{Y}^{(0)} = \mathbf{X} \in
\mathbb{R}^{m\times d}\)</span>，其中 <span
class="math inline">\(\mathbf{X}\)</span> 是原始的特征矩阵。</p>
<p>每一层首先会有一个投影矩阵 (线性层)，接着是一个激活函数。</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y}^{(k)}\)</span>：第 <span
class="math inline">\(k\)</span>
层输出的特征矩阵单个样本的特征向量；</p></li>
<li><p><span class="math inline">\(\mathbf{B}^{(k)}\in
\mathbb{R}^{n_{k-1}\times n_k}\)</span>：第 <span
class="math inline">\((k-1) \to k\)</span>
层的连接矩阵(投影矩阵)；</p></li>
<li><p>$^{(k)} $：第 <span class="math inline">\(k\)</span> 层投影后的第
<span class="math inline">\(j\)</span> 个样本的特征向量，<span
class="math inline">\(\mathbf{z}^{(k)} = {\mathbf{B}^{(k)}}^T
\mathbf{y}^{(k-1)}\)</span>;</p></li>
<li><p><span class="math inline">\(g^{(k)}\)</span>：第<span
class="math inline">\(i\)</span>层的激活函数。对于中间隐藏层来讲，常用的是
ReLU 函数，sigmoid 函数、tanh
函数等。对于输出层，如果是一个分类问题，一般采用softmax；如果是回归问题，则一般不需要激活函数；</p>
<blockquote>
<p>为了方便，对于有 sigmod 和 ReLU 函数，当我们使用 <span
class="math inline">\(g^{(k)}(\mathbf{Z})\)</span>，其表示逐元素运算：
<span class="math display">\[
[g^{(k)}(\mathbf{Z})]_{ij} = g^{(k)}\left([\mathbf{Z}]_{ij}\right).
\]</span> 而对于 softmax 函数，当我们使用 <span
class="math inline">\(g^{(k)}(\mathbf{Y})\)</span>，其表示逐行运算：
<span class="math display">\[
(g^{(k)}(\mathbf{Y}))_i = g^{(k)}\left(\mathbf{Y}_{i}\right).
\]</span></p>
</blockquote></li>
<li><p><span
class="math inline">\(J(\mathbf{Y}^{(l)};\mathbf{Y})\)</span> :
输出层的损失函数。</p></li>
</ul>
<h2 id="network-architecture">Network Architecture</h2>
<p>本文考虑的是简单的全连接神经网络，为了方便，我们不考虑偏置其架构为</p>
<p><strong>前向传播的递推式</strong>：</p>
<ol type="1">
<li><p>线性层 <span class="math display">\[
\mathbf{Z}^{(k)} = \mathbf{Y}^{(k-1)}\mathbf{B}^{(k)}.
\]</span></p></li>
<li><p>激活层 <span class="math display">\[
\mathbf{Y}^{(k)} = g^{(k)}(\mathbf{Z}^{(k)}).
\]</span></p></li>
<li><p>损失函数层：
我们考虑两个目标函数，在分类问题中，我们考虑最小化交叉熵损失：</p>
<p><span class="math display">\[
J(\mathbf{Y}^{(l)};\mathbf{H})  =  -tr\left( \mathbf{H}^T
\log\mathbf{Y}^{(l)}\right) = -\langle \mathbf{H},\log\mathbf{Y}^{(l)}
\rangle.
\]</span> 在回归问题中，我们考虑最小二乘损失： <span
class="math display">\[
J(\mathbf{Y}^{(l)};\mathbf{H}) = \frac{1}{2}\|\mathbf{Y}^{(l)}
-  \mathbf{H}\|_F^2.
\]</span></p>
<blockquote>
<p>注意，为了方便，样本量规范化系数 <span
class="math inline">\(\frac{1}{m}\)</span>
可以暂时不考虑，留到最后计算梯度的时候再考虑进去即可。彼时只需要步长上，令
<span class="math inline">\(\alpha : = \frac{1}{m}\alpha\)</span>
即可。</p>
</blockquote></li>
</ol>
<p>综上，神经网络作为整个映射 <span
class="math inline">\(f(\mathbf{X};\mathbf{\Theta})\)</span>，则可以写成
<span class="math display">\[
\mathbf{Y}^{(l)} =f(\mathbf{X};\mathbf{\Theta})=
g^{(l)}\left(g^{(l-1)}\left(...g^{(2)}\left(g^{(1)}\left(\mathbf{Y}^{(0)}\mathbf{B}^{(1)}
\right)\mathbf{B}^{(2)}  \right)...\right)\mathbf{B}^{(l)} \right).
\]</span> 神经网路的目标在于学习上述映射，即学习各个投影矩阵的参数：
<span class="math display">\[
\mathbf{\Theta} =
(\mathbf{B}^{(1)},\mathbf{B}^{(2)},...,\mathbf{B}^{(l)}).
\]</span></p>
<h2 id="derivation-of-the-gradient">Derivation of the Gradient</h2>
<p>BP
算法的核心，在于通过链式法则推导出了导数的后向传播公式，即高效又方便。因为下本节首先给出了基于链式法则的导数的后向传播公式，我们分别给出了其向量形式和矩阵形式。有了梯度的后向传播之后，我们进一步给出了常用的激活函数的导数，以及两个常用的损失函数的导数。</p>
<h3 id="导数的后向传播公式">导数的后向传播公式</h3>
<h4 id="向量形式">向量形式</h4>
<p>我们的目标是利用损失函数 <span class="math inline">\(J\)</span>
和网络架构，导出损失函数关于各个投影矩阵 <span
class="math inline">\(\mathbf{\Theta} =
(\mathbf{B}^{(1)},\mathbf{B}^{(2)},...,\mathbf{B}^{(l)})\)</span>
的梯度，从而可以用来更新参数。这里参数是投影矩阵 <span
class="math inline">\(\mathbf{B}^{(k)}\)</span>，因此我们需要导出 <span
class="math display">\[
\frac{\partial J}{\partial \mathbf{B}^{(k)}}.
\]</span>
从前向传播式中，我们发现前向传播过程是可以根据样本来分解的，此外，对于目标函数其也是样本可分解的，即
<span class="math inline">\(J(\mathbf{Y}^{(l)};\mathbf{Y}) =
\sum_{i=1}^mJ_i(\mathbf{y}^{(l)};\mathbf{y})\)</span>.
因此求导时也可以根据样本进行分解，即 <span class="math display">\[
\begin{align}
\frac{\partial J}{\partial [\mathbf{B}^{(k)}]_j} &amp; =
\sum_{\mathbf{y}^{(l)}} \frac{\partial J_i}{\partial \mathbf{y}^{(l)}}
\frac{\partial \mathbf{y}^{(l)}}{\partial [\mathbf{B}^{(k)}]_j}.
\end{align}
\]</span> 从而有： <span class="math display">\[
\frac{\partial J}{\partial \mathbf{B}^{(k)}} = \sum_{i = 1}^m
\frac{\partial J_i}{\partial \mathbf{B}^{(k)}}.
\]</span> 在求导的过程中，我们同样引入一些记号。对于第 <span
class="math inline">\(k\)</span> 层，我们记</p>
<ul>
<li><p>损失函数 <span class="math inline">\(J\)</span> 关于第 <span
class="math inline">\(k\)</span> 层中的 <span
class="math inline">\(\mathbf{y}^{(k)}\)</span> 的导数（行向量） <span
class="math display">\[
\mathbf{y}^{(k)}_G =\frac{\partial J}{\partial \mathbf{y}^{(k)}},
\]</span></p></li>
<li><p>损失函数 <span class="math inline">\(J\)</span> 关于第 <span
class="math inline">\(l\)</span> 层中的 <span
class="math inline">\(\mathbf{z}^{(k)}\)</span> 的导数（行向量） <span
class="math display">\[
\mathbf{z}^{(k)}_G =\frac{\partial J}{\partial \mathbf{z}^{(k)}}.
\]</span></p></li>
<li><p>损失函数 <span class="math inline">\(J\)</span> 关于第 <span
class="math inline">\(k\)</span> 层中的 <span
class="math inline">\(\mathbf{B}^{(k)}\)</span> 的导数（矩阵，线性变换）
<span class="math display">\[
\mathbf{B}^{(k)}_G =\frac{\partial J}{\partial \mathbf{B}^{(k)}}.
\]</span></p></li>
<li><p>激活函数 <span class="math inline">\(g^{(k)}\)</span>
往往是逐位操作的，根据激活层公式 <span
class="math inline">\(\mathbf{y}^{(k)} =
g^{(k)}(\mathbf{z}^{(k)})\)</span>，此时的雅各比矩阵 <span
class="math inline">\(\frac{\partial g^{[l]}}{\partial z^{[l]}}\)</span>
是对角矩阵，为了形式方便，我们将其提取为 对角行向量，记之为 <span
class="math inline">\(g_G^{(k)}\)</span>： <span class="math display">\[
g_G^{(k)} = \mathbf{diag}\left(\frac{\partial g^{(k)}}{\partial
\mathbf{z}^{(k)}}\right).
\]</span></p></li>
</ul>
<p>利用向量函数矩阵导数的法则，我们可以利用后一层的求导结果，可以推得前一层的导数。递推式如下：</p>
<ul>
<li><p>损失函数 <span class="math inline">\(J\)</span> 关于第 <span
class="math inline">\(k\)</span> 层中的 <span
class="math inline">\(\mathbf{y}^{(k-1)}\)</span>
的导数（行向量），因为前向传播为： <span class="math display">\[
{\mathbf{z}^{(k)}} = {\mathbf{B}^{(k)}}^T\mathbf{y}^{(k-1)}.
\]</span> 所以 <span class="math display">\[
\frac{\partial {\mathbf{z}^{(k)}}}{\partial {\mathbf{y}^{(k-1)}}}
={\mathbf{B}^{(k)}}^T.
\]</span> 利用链式法则，我们有 <span class="math display">\[
\frac{\partial J}{\partial \mathbf{y}^{(k-1)}} =\frac{\partial
J}{\partial \mathbf{z}^{(k)}}\frac{\partial \mathbf{z}^{(k)}}{\partial
\mathbf{y}^{(k-1)}}.
\]</span> 从而得到 <span class="math display">\[
\mathbf{y}^{(k-1)}_G = {\mathbf{z}^{(k)}_G}{\mathbf{B}^{(k)}}^T.
\]</span></p></li>
<li><p>损失函数 <span class="math inline">\(J\)</span> 关于第 <span
class="math inline">\(l\)</span> 层中的 <span
class="math inline">\(\mathbf{B}^{(k)}\)</span> 的导数（Jacobia
矩阵记法），因为前向传播式为</p>
<p><span class="math display">\[
\mathbf{z}^{(k)} = {\mathbf{B}^{(k)}}^T\mathbf{y}^{(k-1)}.
\]</span>
因为这是一个矩阵到向量的函数，因此它的导数是不好表示的，但是我们可以写成
<span class="math display">\[
{\mathbf{z}^{(k)}}^T = {\mathbf{y}^{(k-1)}}^T{\mathbf{B}^{(k)}}.
\]</span> 从而发现 <span class="math display">\[
\frac{\partial {\mathbf{z}^{(k)}}}{\partial [\mathbf{B}^{(k)}]_j} =
\left[\begin{matrix} 0 \\ \vdots\\ {\mathbf{y}^{(k-1)}}^T
\\   \vdots  \\ 0 \\ \end{matrix}\right] =
\mathbf{e}_j{\mathbf{y}^{(k-1)}}^T.
\]</span> 利用链式法则，我们有<br />
<span class="math display">\[
\frac{\partial J}{\partial[\mathbf{B}^{(k)}]_j} = \frac{\partial
J}{\partial \mathbf{z}^{(k)}}\frac{\partial \mathbf{z}^{(k)}}{\partial
[\mathbf{B}^{(k)}]_j} = \mathbf{z}^{(k)}_G
\mathbf{e}_j{\mathbf{y}^{(k)}}^T = [
\mathbf{z}^{(k)}_G]_j{\mathbf{y}^{(k-1)}}^T.
\]</span> 将上式推广到整个矩阵 <span
class="math inline">\(\mathbf{B}^{(k)}\)</span> 上，可以得到</p>
<p><span class="math display">\[
\begin{align}
&amp;\frac{\partial J}{\partial \mathbf{B}^{(k)}} \\
=&amp; \left[\begin{matrix} {\frac{\partial
J}{\partial[\mathbf{B}^{(k)}]_1} } \\ \vdots \\ {\frac{\partial
J}{\partial[\mathbf{B}^{(k)}]_{n_k}}} \end{matrix}\right] \\
= &amp;  \left[\begin{matrix} [
\mathbf{z}^{(k)}_G]_1{\mathbf{y}^{(k-1)}}^T \\   \vdots \\
[\mathbf{z}^{(k)}_G]_{n_k}{\mathbf{y}^{(k-1)}}^T\end{matrix}\right] \\
=&amp; {\mathbf{z}^{(k)}_G}^T{\mathbf{y}^{(k-1)}}^T.
\end{align}
\]</span> 从而得到 <span class="math display">\[
\mathbf{B}^{(k)}_G =  {\mathbf{z}^{(k)}_G}^T{\mathbf{y}^{(k-1)}}^T.
\]</span></p></li>
<li><p>损失函数 <span class="math inline">\(J\)</span> 关于第 <span
class="math inline">\(k\)</span> 层中的 <span
class="math inline">\(\mathbf{z}^{(k)}\)</span>
的导数（行向量），因为前向传播公式为 <span class="math display">\[
\mathbf{y}^{(k)} = g^{(k)}(\mathbf{z}^{(k)}).
\]</span> 所以 <span class="math display">\[
\frac{\partial {\mathbf{y}^{(k)}}}{\partial {\mathbf{z}^{(k)}}} =
\frac{\partial g^{(k)}}{\partial \mathbf{z}^{(k)}}.
\]</span> 利用链式法则，我们有 <span class="math display">\[
\frac{\partial J}{\partial \mathbf{z}^{(k)}} =\frac{\partial J}{\partial
\mathbf{y}^{(k)}}\frac{\partial \mathbf{y}^{(k)}}{\partial
\mathbf{z}^{(k)}} ={\mathbf{y}^{(k)}_G}\frac{\partial g^{(k)}}{\partial
\mathbf{z}^{(k)}}.
\]</span> 由于激活函数 <span class="math inline">\(g^{(k)}\)</span>
往往是逐位操作的，此时的雅各比矩阵<span
class="math inline">\(\frac{\partial g^{[l]}}{\partial
z^{[l]}}\)</span>是对角矩阵，因此相乘可以变成点乘对角行向量，从而得到
<span class="math display">\[
\mathbf{z}^{(k)}_G = {\mathbf{y}^{(k)}_G}\odot g_G^{(k)}.
\]</span></p></li>
</ul>
<p>上述三个式子称为后向传播递推式，其总结如下 <span
class="math display">\[
\begin{align}
&amp; \mathbf{y}^{(k-1)}_G = {\mathbf{z}^{(k)}_G}{\mathbf{B}^{(k)}}^T ;
\\
&amp; \mathbf{B}^{(k)}_G
=  {\mathbf{z}^{(k)}_G}^T{\mathbf{y}^{(k-1)}}^T; \\
&amp;\mathbf{z}^{(k-1)}_G = {\mathbf{y}^{(k-1)}_G}\odot g_G^{(k-1)}.
\end{align}
\]</span> 这三个式子告诉我们，只要求出输出层第 <span
class="math inline">\(l\)</span> 层的梯度 $
^{(l)}_G$，则后续的梯度均可以通过三个后向传播式进行计算。</p>
<h4 id="矩阵形式">矩阵形式</h4>
<p>我们将上述后向传播的向量形式进一步写成矩阵形式，这会更方便进行代码实现。</p>
<p>对于 <span class="math inline">\(\mathbf{y}^{(k-1)}_G =
{\mathbf{z}^{(k)}_G}{\mathbf{B}^{(k)}}^T\)</span>，其标准矩阵记法为
<span class="math display">\[
\mathbf{Y}^{(k-1)}_G = {\mathbf{Z}^{(k)}_G}{\mathbf{B}^{(k)}}^T.
\]</span> 而对于 <span class="math inline">\(\mathbf{B}^{(k)}_G =
{\mathbf{z}^{(k)}_G}^T{\mathbf{y}^{(k-1)}}^T\)</span>，其对应的标准矩阵记法为
<span class="math display">\[
\mathbf{B}^{(k)}_G =  {\mathbf{Z}^{(k)}_G}^T{\mathbf{Y}^{(k-1)}}.
\]</span> 而对于 <span class="math inline">\(\mathbf{z}^{(k-1)}_G =
{\mathbf{y}^{(k-1)}_G}\odot g_G^{(k-1)}\)</span>，其标准矩阵记法为 <span
class="math display">\[
\mathbf{Z}^{(k-1)}_G = {\mathbf{Y}^{(k-1)}_G}\odot \mathbf{G}_G^{(k-1)}.
\]</span> 其中 <span class="math display">\[
\mathbf{G}_G^{(k-1)} = \left[ \begin{matrix}
g_G^{(k-1)}(\mathbf{Y}_1^{(k-1)}) \\ \vdots\\
g_G^{(k-1)}(\mathbf{Y}_i^{(k-1)}) \\ \vdots \\
g_G^{(k-1)}(\mathbf{Y}_{m}^{(k-1)})\end{matrix}\right].
\]</span> 综上，标准矩阵记法下后向传播公式为: <span
class="math display">\[
\begin{align}
&amp; \mathbf{B}^{(k)}_G =  {\mathbf{Z}^{(k)}_G}^T{\mathbf{Y}^{(k-1)}};
\\
&amp; \mathbf{Y}^{(k-1)}_G = {\mathbf{Z}^{(k)}_G}{\mathbf{B}^{(k)}}^T;
\\
&amp; \mathbf{Z}^{(k-1)}_G = {\mathbf{Y}^{(k-1)}_G}\odot
\mathbf{G}_G^{(k-1)}.
\end{align}
\]</span></p>
<h3 id="激活函数的导数">激活函数的导数</h3>
<p>有了导数的后向传播公式，我们还需要根据具体的激活函数计算具体的求导细则。常用的激活函数主要有三个，其函数形式和导数分别如下：</p>
<ul>
<li><p>ReLU 激活函数： <span class="math display">\[
g(x) = \max\{x,0\};
\]</span> 导数为 <span class="math display">\[
g_G(x) = \mathbf{1}(x&gt;0).
\]</span></p></li>
<li><p>sigmoid 激活函数： <span class="math display">\[
g(x) = \frac{\exp(x)}{1+\exp(x)};
\]</span> 导数为： <span class="math display">\[
g_G(x) = \frac{\exp(x)}{1+\exp(x)}\frac{1}{1+\exp(x)} = g(x)(1-g(x)) =
g(x)g(-x).
\]</span></p></li>
<li><p>tanh 激活函数： <span class="math display">\[
g(x) = \frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)};
\]</span></p>
<p>导数为： <span class="math display">\[
g_G(x)  = 1 - g(x)^2.
\]</span></p></li>
</ul>
<h3 id="损失函数的导数">损失函数的导数</h3>
<p>作为后向传播的第一层输入，损失函数的导数计算非常重要。记 <span
class="math inline">\(\mathbf{Z}^{(l)} = \mathcal{f}(\mathbf{X},
\theta)\)</span> 就是神经网络学习得到特征. 假如 <span
class="math inline">\(\mathbf{Y}\)</span>
是我们的监督信息，目标函数的抽象表达为： <span class="math display">\[
\min_{\mathbf{Z}^{(\ell)}} \ \mathcal{J}(\mathbf{Z}^{(\ell)},
\mathbf{Y}).
\]</span> 在回归网络（自编码器）中，目标函数一般为最小二乘损失： <span
class="math display">\[
\mathcal{J}(\mathbf{Z}^{(l)},\mathbf{Y}) =   \frac{1}{2}\|\mathbf{Y}-
\mathbf{Z}^{(l)}\|_F^2 =  \frac{1}{2}\|\mathbf{Y}-
\mathbf{Y}^{(l)}\|_F^2.
\]</span> 而在分类网络中，目标函数为最小交叉熵损失： <span
class="math display">\[
\begin{align} \mathcal{J}(\mathbf{Z}^{(l)},\mathbf{Y}) =&amp;  -
\left\langle \mathbf{Y},\mathrm{softmax}(\mathbf{Z}^{(l)})\right\rangle
\\ =&amp; - \left\langle \mathbf{Y},\log
\mathrm{Exp_1}(\mathbf{Z}^{(l)})\right\rangle\\ =&amp;  - \left\langle
\mathbf{Y}, \log\mathbf{Y}^{(l)}\right\rangle.\end{align}
\]</span> 根据下列的讨论，我们将看到两个目标函数具有统一的导数，如下：
<span class="math display">\[
\frac{\partial  \mathcal{J}}{\partial \mathbf{Z}^{(l)}} =
\mathbf{Y}^{(l)} - \mathbf{Y}.
\]</span></p>
<h4 id="最小二乘损失的导数">最小二乘损失的导数</h4>
<p>记回归矩阵为 <span class="math inline">\(\mathbf{Y}\)</span>
，特别地，在自回归（自编码）任务中，<span
class="math inline">\(\mathbf{Y} = \mathbf{X}\)</span>.
对于其损失函数，最直接地，可以通过输出值与样本标签值的<strong>距离</strong>定义损失函数，当我们采用欧式距离时，损失函数为：
<span class="math display">\[
\begin{align}
&amp;J(\mathbf{Y}^{(l)};\mathbf{Y}) = \frac{1}{2m}\sum_{i =
1}^{m}(y^{(i)}-\hat{y}^{(i)})^2. \\
&amp;J(\mathbf{Y}^{(l)};\mathbf{Y}) = \frac{1}{2}\|\mathbf{Y}^{(l)}
-  \mathbf{Y}\|_F^2.
\end{align}
\]</span> 从而 <span class="math display">\[
\begin{align}
&amp;\frac{\partial J(\mathbf{Y}^{(l)};\mathbf{Y})}{\partial
\mathbf{Y}^{(l)}} = \mathbf{Y}^{(l)}-\mathbf{Y}.\\
&amp;\mathbf{y}^{(l)}_G = \frac{\partial
J(\mathbf{y}^{(l)};\mathbf{y})}{\partial \mathbf{y}^{(l)}} =
(\mathbf{y}^{(l)}-\mathbf{y})^T.
\end{align}
\]</span>
由于回归任务中，一般回归的范围是整个实数域，所以无须设置激活函数，从而：
<span class="math display">\[
\mathbf{z}^{(l)}_G = \mathbf{y}_G^{(l)}  =
(\mathbf{y}^{(l)}-\mathbf{y})^T.
\]</span></p>
<h4 id="最小交叉熵损失的导数">最小交叉熵损失的导数</h4>
<p>记有 r 个类别，概率标签矩阵为 <span
class="math inline">\(\mathbf{Y}\)</span>，则输出矩阵 <span
class="math inline">\(\mathbf{Y}^{(l)}\)</span>
一般是一个离散概率分布列矩阵，那么可以根据似然原理或 KL
散度定义损失函数： <span class="math display">\[
\begin{align}
&amp;J(\mathbf{Y}^{(l)};\mathbf{Y}) =\frac{1}{m} \sum_{i = 1}^{m}\sum_{j
= 1}^{k}y_j^{(i)}\log(\hat{y}_j^{(i)}). \\
&amp;J(\mathbf{Y}^{(l)};\mathbf{Y}) = -   tr\left( \mathbf{Y}^T
\log\mathbf{Y}^{(l)}\right)  =  - \left\langle \mathbf{Y},
\log\mathbf{Y}^{(l)}\right\rangle.
\end{align}
\]</span> 从而有 <span class="math display">\[
\begin{align}
&amp;J(\mathbf{y}^{(l)};\mathbf{y}) = -\mathbf{y}^T\log\mathbf{y}^{(l)}.
\\
&amp;\frac{\partial J(\mathbf{y}^{(l)};\mathbf{y})}{\partial
\mathbf{y}^{(l)}} = -
\left(\mathbf{y}\odot\frac{1}{\mathbf{y}^{(l)}}\right)^T.
\end{align}
\]</span> 其中 <span class="math display">\[
\left[\frac{1}{\mathbf{Y}^{(l)}}\right]_{ij} =
\frac{1}{\left[\mathbf{Y}^{(l)}\right]_{ij}}.
\]</span> 从而得到 <span class="math display">\[
\mathbf{y}_G^{(l)} =  -\mathbf{y}^T\frac{1}{\mathbf{y}^{(l)}}.
\]</span> 而对于 $_G^{(l)} $，我们知道有</p>
<p><span class="math display">\[
\mathbf{z}_G^{(l)} =
{\mathbf{y}^{(l)}_G}\frac{\partial g^{(l)}}
{\partial \mathbf{z}^{(l)}}.
\]</span></p>
<p>由于分类任务中，有效取值为 [ 0 , 1 ] 区间，因此
<strong>最后一层激活函数</strong> 可以设置为 softmax 激活函数，其定义为
( 正值化 + 归一化 )</p>
<p><span class="math display">\[
\mathbf{y}^{(l)}_j = [g^{(l)}(\mathbf{z}^{(l)})]_j =
\frac{\exp{(\mathbf{z}^{(l)}_j)}}{\sum_{t=1}^r\exp{(\mathbf{z}^{(l)}_t)}}.
\]</span></p>
<p>所以，对于 $ i j$，有 <span class="math display">\[
\begin{align}
\frac{\partial \mathbf{y}^{(l)}_j}{\partial \mathbf{z}_i^{(l)}} = &amp;
\frac{
0-\exp{\left(\mathbf{z}^{(l)}_j\right)}\exp{\left(\mathbf{z}^{(l)}_i\right)}}{\left[\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}\right]^2}  \\
= &amp; -\frac{
\exp{\left(\mathbf{z}^{(l)}_j\right)}}{\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}}\frac{
\exp{\left(\mathbf{z}^{(l)}_i\right)}}{\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}}
\\
&amp; = \mathbf{y}^{(l)}_j (-\mathbf{y}^{(l)}_i ).
\end{align}
\]</span> 而对于 <span class="math inline">\(i = j\)</span>，有 <span
class="math display">\[
\begin{align}
\frac{\partial \mathbf{y}^{(l)}_j}{\partial \mathbf{z}_j^{(l)}} = &amp;
\frac{
\exp{\left(\mathbf{z}^{(l)}_j\right)}\left[\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}\right]-\exp{\left(\mathbf{z}^{(l)}_j\right)}\exp{\left(\mathbf{z}^{(l)}_j\right)}}{\left[\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}\right]^2}  \\
= &amp; \frac{ \exp{\left(\mathbf{z}^{(l)}_j\right)}\left[\sum_{t\neq
j}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}\right]}{\left[\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}\right]^2}
\\
= &amp; \frac{
\exp{\left(\mathbf{z}^{(l)}_j\right)}}{\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}}\frac{
\sum_{t\neq
j}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}}{\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}}
\\
= &amp;\frac{
\exp{\left(\mathbf{z}^{(l)}_j\right)}}{\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}}\left(1-\frac{
\exp{\left(\mathbf{z}^{(l)}_j\right)}}{\sum_{t=1}^r\exp{\left(\mathbf{z}^{(l)}_t\right)}}
\right) \\
= &amp; \mathbf{y}^{(l)}_j (1-\mathbf{y}^{(l)}_j  ) .
\end{align}
\]</span> 综上，得到 <span class="math display">\[
\frac{\partial \mathbf{y}^{(l)}_j}{\partial \mathbf{z}^{(l)}} =
\mathbf{y}^{(l)}_j(\mathbf{e_j}-\mathbf{y}^{(l)})^T.
\]</span> 从而 <span class="math display">\[
\frac{\partial \mathbf{y}^{(l)}}{\partial \mathbf{z}^{(l)}} =
\frac{\partial g^{(l)}}{\partial \mathbf{z}^{(l)}}
=  (\mathbf{y}^{(l)}\mathbf{1}^T)\odot\left(\mathbf{I}-\mathbf{1}{\mathbf{y}^{(l)}}^T\right).
\]</span> 因此 <span class="math display">\[
\begin{align}
\mathbf{z}_G^{(l)} = &amp; {\mathbf{y}^{(l)}_G}\frac{\partial
g^{(l)}}{\partial \mathbf{z}^{(l)}}  \\
=  &amp; -
\left(\mathbf{y}\odot\frac{1}{\mathbf{y}^{(l)}}\right)^T\left[(\mathbf{y}^{(l)}\mathbf{1}^T)\odot\left(\mathbf{I}-\mathbf{1}{\mathbf{y}^{(l)}}^T\right)\right]
\\
=  &amp; -
\mathbf{diag}\left(\left(\left[\mathbf{y}\odot\frac{1}{\mathbf{y}^{(l)}}\mathbf{1}^T\right]\odot(\mathbf{y}^{(l)}\mathbf{1}^T)\right)^T\left(\mathbf{I}-\mathbf{1}{\mathbf{y}^{(l)}}^T\right)\right)^T\\
&amp; = -\mathbf{diag}
\left(\mathbf{1}\mathbf{y}^T\left(\mathbf{I}-\mathbf{1}{\mathbf{y}^{(l)}}^T\right)\right)^T  \\
&amp; = -
\mathbf{y}^T\left(\mathbf{I}-\mathbf{1}{\mathbf{y}^{(l)}}^T\right) \\
&amp; = {\mathbf{y}^{(l)}}^T -\mathbf{y}^T \\
&amp; = \left({\mathbf{y}^{(l)}} -\mathbf{y}\right)^T.
\end{align}
\]</span></p>
<h2 id="the-schedule-of-batch-stochastic-gradient-descend">The schedule
of Batch Stochastic Gradient Descend</h2>
<p>有了导数后向传播公式，也有了关于目标函数和激活函数的导数，我们就可以利用随机梯度下降算法来进行优化网络参数。我们采用批量梯度下降来优化，批量梯度的思想介于全局梯度和随机梯度，其基本是利用以小批量的样本来估计出全局的梯度。相比于全局梯度，其计算量小，而相比于随机梯度，其估计得更加准确，算法更加稳定。</p>
<p>批量梯度下降的基本流程为：</p>
<ol type="1">
<li><p>先计算损失函数的导数。对于分类和回归网络，我们均有 <span
class="math display">\[
\mathbf{z}^{(l)}_G = \left({\mathbf{y}^{(l)}} -\mathbf{y}\right)^T.
\]</span> 写成标准矩阵的形式，则为 <span class="math display">\[
\mathbf{Z}^{(l)}_G = {\mathbf{Y}^{(l)}} -\mathbf{Y}.
\]</span></p></li>
<li><p>根据后向传播公式，计算目标函数关于网络连接权重（投影矩阵） <span
class="math inline">\(\mathbf{B}^{(k)}\)</span> 的导数：<span
class="math inline">\(\mathbf{B}^{(k)}_G\)</span>. <span
class="math display">\[
\begin{align}
&amp;\mathbf{B}^{(k)}_G = {\mathbf{Z}^{(k)}_G}^T{\mathbf{Y}^{(k-1)}}.\\
&amp;\mathbf{Y}^{(k-1)}_G = {\mathbf{Z}^{(k)}_G}{\mathbf{B}^{(k)}}^T.\\
&amp;\mathbf{Z}^{(k-1)}_G = {\mathbf{Y}^{(k-1)}_G}\odot
\mathbf{G}_G^{(k-1)}.
  \end{align}
\]</span></p></li>
<li><p>根据学习率 <span
class="math inline">\(\alpha\)</span>，进行梯度下降： <span
class="math display">\[
\mathbf{B}^{(k)} := \mathbf{B}^{(k)} - \alpha \ {\mathbf{B}^{(k)}_G}^T.
\]</span></p></li>
</ol>
<h2 id="python-implementation-trick">Python Implementation Trick</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/Volcano-X/BPNet">python 实现代码</a></p>
<ol type="1">
<li><p>充分利用矢量化计算的速度</p>
<p>BPNet
的训练中涉及到比较多的计算，直接利用程序的循环结构来实现迭代是比较慢的，因此需要尽可能地利用矩阵乘法和一些矢量化计算，因此这些函数在python
的 numpy 库中是经过优化过的，可以使得训练速度提高。</p></li>
<li><p>解决指数溢出问题 sigmoid 与 softmax 均有指数计算 <span
class="math inline">\(e^x\)</span>，这很容易导致过大溢出，但是可以通过简单的方法完美避开。
对于 softmax 函数，选 <span class="math inline">\(M =
\max\{x_i\}\)</span>，则有 <span class="math display">\[
\frac{e^{x_j}}{1+\sum_{i = 1}^{k-1}e^{x_i}} =
\frac{e^{x_j-M}}{e^{-M}+\sum_{i = 1}^{k-1}e^{x_i-M}}.
\]</span> 此时分子小于等于1，而分母大于1。</p>
<p>对于 sigmoid
函数，他其实是softmax的二维情况，但他有一个更合适的方法：</p>
<ul>
<li>当<span class="math inline">\(x&gt;0\)</span>，利用<span
class="math inline">\(\frac{1}{1+e^{-x}}\)</span>计算；</li>
<li>当<span class="math inline">\(x&lt;0\)</span>，利用<span
class="math inline">\(\frac{e^{x}}{1+e^{x}}\)</span> 计算。</li>
</ul>
<p>对于 <span class="math inline">\(\tanh\)</span>
函数，可以直接利用其与 sigmoid 函数的关系计算 <span
class="math display">\[
\tanh(x) = 2\sigma(2x)-1.
\]</span></p></li>
<li><p>连接权重矩阵的初始化问题 使用 Xavier/He 初始化方法： <span
class="math display">\[
\mathbf{B}^{(k)} \sim N\left(0,\frac{2}{n_{k-1}+n_{k}}\right).
\]</span></p></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/neural-network/" rel="tag"># neural network</a>
              <a href="/tags/science/" rel="tag"># science</a>
              <a href="/tags/technology/" rel="tag"># technology</a>
              <a href="/tags/math/" rel="tag"># math</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/12/07/why-write-blog/" rel="prev" title="为什么要写博客">
      <i class="fa fa-chevron-left"></i> 为什么要写博客
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#notation"><span class="nav-number">2.</span> <span class="nav-text">Notation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#network-architecture"><span class="nav-number">3.</span> <span class="nav-text">Network Architecture</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#derivation-of-the-gradient"><span class="nav-number">4.</span> <span class="nav-text">Derivation of the Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E6%95%B0%E7%9A%84%E5%90%8E%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F"><span class="nav-number">4.1.</span> <span class="nav-text">导数的后向传播公式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%BD%A2%E5%BC%8F"><span class="nav-number">4.1.1.</span> <span class="nav-text">向量形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F"><span class="nav-number">4.1.2.</span> <span class="nav-text">矩阵形式</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="nav-number">4.2.</span> <span class="nav-text">激活函数的导数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="nav-number">4.3.</span> <span class="nav-text">损失函数的导数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="nav-number">4.3.1.</span> <span class="nav-text">最小二乘损失的导数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="nav-number">4.3.2.</span> <span class="nav-text">最小交叉熵损失的导数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#the-schedule-of-batch-stochastic-gradient-descend"><span class="nav-number">5.</span> <span class="nav-text">The schedule
of Batch Stochastic Gradient Descend</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#python-implementation-trick"><span class="nav-number">6.</span> <span class="nav-text">Python Implementation Trick</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiacan Zheng"
      src="/images/zjc-512x512.png">
  <p class="site-author-name" itemprop="name">Jiacan Zheng</p>
  <div class="site-description" itemprop="description">Great Truths Are Always Simple.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/volcano-x" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;volcano-x" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/jiacan_zheng@163.com" title="E-Mail → jiacan_zheng@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiacan Zheng</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
